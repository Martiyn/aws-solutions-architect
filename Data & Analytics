Amazon Athena

 - Athena is a serverless query service to analyze data stored in Amazon S3
 - Uses standard SQL language to query the files (built on Presto)
 - Supports CSV, JSON, ORC, Avro, Parquet
 - Pricing: $5.00 per TB of data scanned
 - Commonly used with Amazon Quicksight for reporting/dashboards
 - Use cases:
    1. Business intelligence
    2. Analytics
    3. Reporting
    4. Analyze & query VPC flow logs
    5. Analyze & query ELB logs
    6. Analyze & query CloudTrail trails
 - Exam Tip: analyze data in S3 using serverless SQL, use Athena

 - Amazon Athena - Performance improvement:
 - Use columnar data for cost-savings (less scan)
 - Recommended formats for this are:
    1. Apache Parquet
    2. ORC
 - You can use Glue to convert your data to Parquet or ORC
 - This provides a huge performance improvement
 - Compress data for smaller retrievals (bzip2, gzip, lz4, snappy, zlip, zstd...)
 - Partition datasets in S3 for easy querying on virtual columns
 - s3://yourBucket/pathToTable
                 /<PARTITION_COLUMN_NAME>=<VALUE>
                   /<PARTITION_COLUMN_NAME>=<VALUE>
                     /<PARTITION_COLUMN_NAME>=<VALUE>
                       /etc...

 - Example: s3://athena-examples/flight/parquet/year=1991/month=1/day=1/
 - Use larger files (> 128 MB) to minimize overhead

 - Amazon Athena - Federated Query:
 - Allows you to run SQL queries across data stored in relational, non-relational, object and custom data sources (AWS or on-prem)
 - User Data Source Connectors that run on AWS Lambda to run Federated Queries (e.g., CloudWatch Logs, DynamoDB, RDS, ...)
 - Store the results back in Amazon S3
 - The Data Source Connector is a Lambda function.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Amazon Redshift

 - Redshift is based on PostgreSQL, but it's not used for onlile transaction processing (OLTP)
 - It is actually for online analytical processing (OLAP)
 - It is used for analytics and data warehousing
 - 10x better performance than other data warehouses, scale to PBs of data
 - Columnar storage of data (instead of row based) & parallel query engine
 - Pay as you go based on provisioned instances
 - Has a SQL interface for performing the queries
 - BI tools such as Amazon Quicksight or Tableau integrate with it
 - vs Athena: faster queries / joins / aggregations thanks to indexes
 - Athena is more useful for small-scale queries on S3
 - Redshift is more useful for intense data warehousing with many complicated queries

 - Redshift cluster:
 - Leader node: for query planning, results aggregation
 - Compute node: for performing the queries, sends results to leader node
 - You provision the node size in advance
 - You can use reserved instances for cost savings

 - Redshift - Snapshots & Disaster Recovery:
 - Redshift has multi-az mode for some clusters
 - Snapshots are point-in-time backups of a cluster, stored internally in S3
 - Snapshots are incremental (only what has changed is saved)
 - You can restore a snapshot into a new cluster
 - Automated: every 8 hours, every 5 GB, or on a schedule. Set retention.
 - Manual: snapshot is retained until you delete it
 - You can configure Amazon Redshift to automatically copy snapshots (automated or manual) of a cluster to another AWS region
 - You can restore a new Redshift cluster from the copied snapshot

 - Loading data into Redshift:
 - Large inserts are MUCH better
 - You can use Amazon Kinesis Data Firehose
 - Firehose will receive data from different sources and then send it to Redshift
 - To do so, it will first write the data in an S3 bucket, after which, Firehose will automatically issue an S3 copy command
 - This will load the data into Redshift
 - You can also issue a copy command manually (requires IAM role)
 - You can also use an EC2 Instance JDBC driver (better for writing large batches of data)

 - Redshift Spectrum:
 - Query data that is already in S3 without loading it
 - Must have a Redshift cluster available to start the query
 - The query is then submitted to thousands of Redshift Spectrum nodes

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Amazon OpenSearch Service

 - Amazon OpenSearch is a successor to Amazon ElasticSearch
 - In DynamoDB, queries only exist by primary key or indexes
 - With OpenSearch, you can search any field, even partial matches
 - It is common to use OpenSearch as a compliment to another database
 - Two modes: managed cluster or serverless cluster
 - Does not natively support SQL (SQL support can be enabled via plugin)
 - Ingestion from Kinesis Data Firehose, AWS IoT, CloudWatch Logs
 - Security through Cognito & IAM, KMS encryption, TLS
 - Comes with OpenSearch Dashboards (visualization)

 - OpenSearch patterns (DynamoDB):

   ---(CRUD)---> DynamoDB Table ------> DynamoDB Stream ------> Lambda Function ------> Amazon OpenSearch

 - Per the above, your application can perform a partial search (to search for a specific item)
 - Item can be searched via name or another attribute and then the item ID will be found
 - Once the item ID is found, it will be used to call DynamoDB to retrieve the full item

 - OpenSearch patterns (CloudWatch Logs):

   CloudWatch Logs ------> Subscription Filter ------> Lambda Function ---(Real Time)---> Amazon OpenSearch
 - or
   CloudWatch Logs ------> Subscription Filter ------> Kinesis Data Firehose ---(Near Real Time)---> Amazon OpenSearch

 - OpenSearch patterns (Kinesis Data Streams & Kinesis Data Firehose)

   Kinesis Data Streams ------> Kinesis Data Firehose ---(Optionally transform data with Lambda func.)---> Amazon OpenSearch
 - or
   Kinesis Data Streams ------> Lambda Function ------> Amazon OpenSearch

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Amazon EMR

 - EMR stands for Elastic MapReduce
 - EMR helps with the creation of Hadoop clusters (big data) to analyze and process vast amounts of data
 - The clusters can be made of hundreds of EC2 instances
 - EMR comes bundled with Apache Spark, HBase, Presto, Flink
 - EMR takes care of all the provisioning and configuration
 - Auto-scaling and integrated with Spot instances
 - Use cases:
    1. Data processing
    2. Machine learning
    3. Web indexing
    4. Big data

 - Amazon EMR - Node types & purchasing
 - Master node: manage the cluster, coordinate, manage health - long running
 - Core node: run tasks and store data - long running
 - Task node (optional): just to run tasks - usually Spot instances
 - Purchasing options:
    1. On-demand: reliable, predictable, won't be terminated
    2. Reserved (min 1 year): cost savings (EMR will automatically use if available)
    3. Spot instances: cheaper, can be terminated, less reliable
 - Can have long-running cluster or transient (temporary) cluster

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Amazon QuickSight

 - Amazon QuickSight is a serverless machine learning-powered business intelligence service to create interactive dashboards
 - Dashboards are connected to data sources you own
 - Fast, automatically scalable, embeddable, with per-session pricing
 - Use cases:
    1. Business analytics
    2. Building visualizations
    3. Perform ad-hoc analysis
    4. Get business insights using data
 - Integrated with RDS, Aurora, Athena, Redshift, S3, ...
 - In-memory computation using SPICE engine if data is imported into QuickSight
 - Enterprise edition: possibility to setup column-level securitu (CLS)

 - QuickSight integrations:
 - Data sources (AWS Services):
    1. RDS
    2. Aurora
    3. Redshift
    4. Athena
    5. S3
    6. OpenSearch
    7. Timestream
 - Data sources (3rd party SaaS):
    1. Salesforce
    2. Jira
    3. etc...
 - Data sources: on-premises DB (JDBC)
 - Data sources (imports):
    1. .XLSX
    1. .CSV
    1. .JSON
    1. .TSV
    1. ELF & CLF (log format)

 - QuickSight - Dashboard & Analysis:
 - Define users (standard versions) and Groups (enterprise version)
    1. These users & groups only exist within QuickSight, not IAM !!
 - A dashboard:
    1. Is a read-only snapshot of an analysis that you can share
    2. Preserves the configuration of the analysis (filtering, parameters, controls, sort)
 - You can share the analysis or the dashboard with Users or Groups
 - To share a dashboard, you must first publish it
 - Users who see the dashboard can also see the underlying data

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

AWS Glue

 - AWS Glue is a managed extract, transform, and load (ETL) service
 - Useful to prepare and transform data for analytics
 - Fully serverless service
 - Example diagram:

   S3 bucket/Amazon RDS ---(Extract)---> Glue ETL ---(Load)---> RedShift Data Warehouse

 - AWS Glue - Convert data into Parquet format:

   ---(S3 put)---> Input S3 bucket ---(Import CSV)---> Glue ETL ---(Parquet)---> Output S3 bucket ---(Analyze)---> Amazon Athena

 - To automate the process, you can place event notifications on S3 put, which would trigger a Lambda function or EventBridge to trigger a Glue ETL job.

 - Glue Data Catalog: catalog of datasets:

   Amazon S3/Amazon RDS/Amazon DynamoDB/On-prem (JDBC) ------> AWS Glue Data Crawler ---(writes metadata)---> AWS Glue Data Catalog ---(data discovery)---> Amazon Athena/Amazon Redshift Spectrum/Amazon EMR

 - Glue - things to know at a high level:
 - Glue Job Bookmarks: prevent re-processing of old data
 - Glue Elastic Views:
    1. Combine and replicate data across multiple data stores using SQL
    2. No custom code, Glue monitors for changes in the source data, serverless
    3. Leverages a "virtual table" (materialized view)
 - Glue DataBrew: clean and normalize data using pre-built transformation
 - Glue Studio: new GUI to create, run and monitor ETL jobs in Glue
 - Glue Streaming ETL (built on Apache Spark Structured Streaming): compatible with Kinesis Data Streaming, Kafka, MSK (managed Kafka)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

