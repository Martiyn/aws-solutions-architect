Amazon Athena

 - Athena is a serverless query service to analyze data stored in Amazon S3
 - Uses standard SQL language to query the files (built on Presto)
 - Supports CSV, JSON, ORC, Avro, Parquet
 - Pricing: $5.00 per TB of data scanned
 - Commonly used with Amazon Quicksight for reporting/dashboards
 - Use cases:
    1. Business intelligence
    2. Analytics
    3. Reporting
    4. Analyze & query VPC flow logs
    5. Analyze & query ELB logs
    6. Analyze & query CloudTrail trails
 - Exam Tip: analyze data in S3 using serverless SQL, use Athena

 - Amazon Athena - Performance improvement:
 - Use columnar data for cost-savings (less scan)
 - Recommended formats for this are:
    1. Apache Parquet
    2. ORC
 - You can use Glue to convert your data to Parquet or ORC
 - This provides a huge performance improvement
 - Compress data for smaller retrievals (bzip2, gzip, lz4, snappy, zlip, zstd...)
 - Partition datasets in S3 for easy querying on virtual columns
 - s3://yourBucket/pathToTable
                 /<PARTITION_COLUMN_NAME>=<VALUE>
                   /<PARTITION_COLUMN_NAME>=<VALUE>
                     /<PARTITION_COLUMN_NAME>=<VALUE>
                       /etc...

 - Example: s3://athena-examples/flight/parquet/year=1991/month=1/day=1/
 - Use larger files (> 128 MB) to minimize overhead

 - Amazon Athena - Federated Query:
 - Allows you to run SQL queries across data stored in relational, non-relational, object and custom data sources (AWS or on-prem)
 - User Data Source Connectors that run on AWS Lambda to run Federated Queries (e.g., CloudWatch Logs, DynamoDB, RDS, ...)
 - Store the results back in Amazon S3
 - The Data Source Connector is a Lambda function.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Amazon Redshift

 - Redshift is based on PostgreSQL, but it's not used for onlile transaction processing (OLTP)
 - It is actually for online analytical processing (OLAP)
 - It is used for analytics and data warehousing
 - 10x better performance than other data warehouses, scale to PBs of data
 - Columnar storage of data (instead of row based) & parallel query engine
 - Pay as you go based on provisioned instances
 - Has a SQL interface for performing the queries
 - BI tools such as Amazon Quicksight or Tableau integrate with it
 - vs Athena: faster queries / joins / aggregations thanks to indexes
 - Athena is more useful for small-scale queries on S3
 - Redshift is more useful for intense data warehousing with many complicated queries

 - Redshift cluster:
 - Leader node: for query planning, results aggregation
 - Compute node: for performing the queries, sends results to leader node
 - You provision the node size in advance
 - You can use reserved instances for cost savings

 - Redshift - Snapshots & Disaster Recovery:
 - Redshift has multi-az mode for some clusters
 - Snapshots are point-in-time backups of a cluster, stored internally in S3
 - Snapshots are incremental (only what has changed is saved)
 - You can restore a snapshot into a new cluster
 - Automated: every 8 hours, every 5 GB, or on a schedule. Set retention.
 - Manual: snapshot is retained until you delete it
 - You can configure Amazon Redshift to automatically copy snapshots (automated or manual) of a cluster to another AWS region
 - You can restore a new Redshift cluster from the copied snapshot

 - Loading data into Redshift:
 - Large inserts are MUCH better
 - You can use Amazon Kinesis Data Firehose
 - Firehose will receive data from different sources and then send it to Redshift
 - To do so, it will first write the data in an S3 bucket, after which, Firehose will automatically issue an S3 copy command
 - This will load the data into Redshift
 - You can also issue a copy command manually (requires IAM role)
 - You can also use an EC2 Instance JDBC driver (better for writing large batches of data)

 - Redshift Spectrum:
 - Query data that is already in S3 without loading it
 - Must have a Redshift cluster available to start the query
 - The query is then submitted to thousands of Redshift Spectrum nodes

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Amazon OpenSearch Service

 - Amazon OpenSearch is a successor to Amazon ElasticSearch
 - In DynamoDB, queries only exist by primary key or indexes
 - With OpenSearch, you can search any field, even partial matches
 - It is common to use OpenSearch as a compliment to another database
 - Two modes: managed cluster or serverless cluster
 - Does not natively support SQL (SQL support can be enabled via plugin)
 - Ingestion from Kinesis Data Firehose, AWS IoT, CloudWatch Logs
 - Security through Cognito & IAM, KMS encryption, TLS
 - Comes with OpenSearch Dashboards (visualization)

 - OpenSearch patterns (DynamoDB):

   ---(CRUD)---> DynamoDB Table ------> DynamoDB Stream ------> Lambda Function ------> Amazon OpenSearch

 - Per the above, your application can perform a partial search (to search for a specific item)
 - Item can be searched via name or another attribute and then the item ID will be found
 - Once the item ID is found, it will be used to call DynamoDB to retrieve the full item

 - OpenSearch patterns (CloudWatch Logs):

   CloudWatch Logs ------> Subscription Filter ------> Lambda Function ---(Real Time)---> Amazon OpenSearch
 - or
   CloudWatch Logs ------> Subscription Filter ------> Kinesis Data Firehose ---(Near Real Time)---> Amazon OpenSearch

 - OpenSearch patterns (Kinesis Data Streams & Kinesis Data Firehose)

   Kinesis Data Streams ------> Kinesis Data Firehose ---(Optionally transform data with Lambda func.)---> Amazon OpenSearch
 - or
   Kinesis Data Streams ------> Lambda Function ------> Amazon OpenSearch

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Amazon EMR

 - EMR stands for Elastic MapReduce
 - EMR helps with the creation of Hadoop clusters (big data) to analyze and process vast amounts of data
 - The clusters can be made of hundreds of EC2 instances
 - EMR comes bundled with Apache Spark, HBase, Presto, Flink
 - EMR takes care of all the provisioning and configuration
 - Auto-scaling and integrated with Spot instances
 - Use cases:
    1. Data processing
    2. Machine learning
    3. Web indexing
    4. Big data

 - Amazon EMR - Node types & purchasing
 - Master node: manage the cluster, coordinate, manage health - long running
 - Core node: run tasks and store data - long running
 - Task node (optional): just to run tasks - usually Spot instances
 - Purchasing options:
    1. On-demand: reliable, predictable, won't be terminated
    2. Reserved (min 1 year): cost savings (EMR will automatically use if available)
    3. Spot instances: cheaper, can be terminated, less reliable
 - Can have long-running cluster or transient (temporary) cluster

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Amazon QuickSight

 - Amazon QuickSight is a serverless machine learning-powered business intelligence service to create interactive dashboards
 - Dashboards are connected to data sources you own
 - Fast, automatically scalable, embeddable, with per-session pricing
 - Use cases:
    1. Business analytics
    2. Building visualizations
    3. Perform ad-hoc analysis
    4. Get business insights using data
 - Integrated with RDS, Aurora, Athena, Redshift, S3, ...
 - In-memory computation using SPICE engine if data is imported into QuickSight
 - Enterprise edition: possibility to setup column-level securitu (CLS)

 - QuickSight integrations:
 - Data sources (AWS Services):
    1. RDS
    2. Aurora
    3. Redshift
    4. Athena
    5. S3
    6. OpenSearch
    7. Timestream
 - Data sources (3rd party SaaS):
    1. Salesforce
    2. Jira
    3. etc...
 - Data sources: on-premises DB (JDBC)
 - Data sources (imports):
    1. .XLSX
    1. .CSV
    1. .JSON
    1. .TSV
    1. ELF & CLF (log format)

 - QuickSight - Dashboard & Analysis:
 - Define users (standard versions) and Groups (enterprise version)
    1. These users & groups only exist within QuickSight, not IAM !!
 - A dashboard:
    1. Is a read-only snapshot of an analysis that you can share
    2. Preserves the configuration of the analysis (filtering, parameters, controls, sort)
 - You can share the analysis or the dashboard with Users or Groups
 - To share a dashboard, you must first publish it
 - Users who see the dashboard can also see the underlying data

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

AWS Glue

 - AWS Glue is a managed extract, transform, and load (ETL) service
 - Useful to prepare and transform data for analytics
 - Fully serverless service
 - Example diagram:

   S3 bucket/Amazon RDS ---(Extract)---> Glue ETL ---(Load)---> RedShift Data Warehouse

 - AWS Glue - Convert data into Parquet format:

   ---(S3 put)---> Input S3 bucket ---(Import CSV)---> Glue ETL ---(Parquet)---> Output S3 bucket ---(Analyze)---> Amazon Athena

 - To automate the process, you can place event notifications on S3 put, which would trigger a Lambda function or EventBridge to trigger a Glue ETL job.

 - Glue Data Catalog: catalog of datasets:

   Amazon S3/Amazon RDS/Amazon DynamoDB/On-prem (JDBC) ------> AWS Glue Data Crawler ---(writes metadata)---> AWS Glue Data Catalog ---(data discovery)---> Amazon Athena/Amazon Redshift Spectrum/Amazon EMR

 - Glue - things to know at a high level:
 - Glue Job Bookmarks: prevent re-processing of old data
 - Glue Elastic Views:
    1. Combine and replicate data across multiple data stores using SQL
    2. No custom code, Glue monitors for changes in the source data, serverless
    3. Leverages a "virtual table" (materialized view)
 - Glue DataBrew: clean and normalize data using pre-built transformation
 - Glue Studio: new GUI to create, run and monitor ETL jobs in Glue
 - Glue Streaming ETL (built on Apache Spark Structured Streaming): compatible with Kinesis Data Streaming, Kafka, MSK (managed Kafka)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

AWS Lake Formation

 - Data lake = central place to have all your data for analytics purposes
 - Lake formation is a fully managed service that makes it easy to setup a data lake in days
 - Discover, cleanse, transform, and ingest data into your Data Lake
 - It automates many complex manual steps (collecting, cleansing, mpving, cataloging data, ...) and de-duplicate (using ML Transforms)
 - Combine structured and unstructured data in the data lake
 - Out-of-the-box source blueprints: S3, RDS, Relational & NoSQL DB, ...
 - Fine-grained Access Control for your applications (row and column-level)
 - Built on top of AWS Glue (does not interact with it directly)
 - The data lake is stored in Amazon S3
 - Data sources can be:
    1. Amazon S3
    1. RDS
    1. Aurora
    1. On-prem. DB (SQL & NoSQL)
 - The lake formation ingests the data from the above data sources
 - The data lake contains the following:
    1. Source crawlers
    2. ETL and Data prep.
    3. Data catalog
    4. Security settings
    5. Access control
 - Services that can leverage Lake formation are:
    1. Athena
    2. Redshift
    3. EMR
    4. Apache Spark (non-AWS)
 - Users connect to the above services, which in turn connect to Data Lake
 - Lake formation allows you to setup access control (column & row-level security) from within the Lake Formation itself
 - In that way, any service that connects to Lake Formation is allowed to view data that is allowed by the Lake Formation access control
 - This centralizes permissions (makes them easier to manage)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Amazon Kinesis Data Analytics

 - There are two types of Kinesis Data Analytics:
    1. For SQL applications
    2. For Apache Flink

 - Kinesis Data Analytics for SQL applications
 - It has two data sources that it can read from:
    1. Kinesis Data Streams
    2. Kinesis Data Firehose
 - You can apply SQL statements to perform real-time analytics
 - You can join reference data by referencing it from an S3 bucket (allows you to enrich data in real-time)
 - You can send the data to the following destinations:
    1. Kinesis Data Streams (can do real-time processing of the stream with Lambda or personal applications)
    2. Kinesis Data Firehose (can send data to the usual Firehose destinations)
 - Real-time analytics on Kinesis Data Streams & Firehose using SQL
 - Add reference data from Amazon S3 to enrich streaming data
 - Fully managed, no servers to provision
 - Automatic scaling
 - Pay for actual consumption rate
 - Output:
    1. Kinesis Data Streams: create streams out of the real-time analytics queries
    2. Kinesis Data Firehose: send analytics query results to destinations
 - Use cases:
    1. Time-series analytics
    2. Real-time dashboards
    3. Real-time metrics
    
 - Kinesis Data Analytics for Apache Flink (Amazon Managed Service for Apache Flink):
 - Use Flink (Java, Scala or SQL) to process and analyze streaming data
 - You can read from two data sources:
    1. Kinesis Data Streams
    2. Amazon MSK
 - Flink is used for running very complicated queries (above the capabilities of SQL)
 - With this service, you get:
    1. Provisioning compute resources, parallel computation, automatic scaling
    2. Application backups (implemented as checkpoints and snapshots)
    3. Use any Apache Flink programming features
    4. Flink does not read from Firehose (use Kinesis Analytics for SQL instead)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Amazon Managed Streaming for Apache Kafka (Amazon MSK)

 - Amazon MSK is an alternative to Amazon Kinesis
 - Fully managed Apache Kafka on AWS
 - Allows you to create, update, delete clusters
 - MSK creates & manages Kafka brokers nodes & Zookeeper nodes for you
 - Deploy the MSK cluster in your VPC, multi-AZ (up to 3 for HA)
 - Automatic recovery from common Apache Kafka failures
 - Data is stored on EBS volumes for as long as you want

 - MSK Serverless:
    1. Run Apache Kafka on MSK without managing the capacity
    2. MSK automatically provisions resources and scales compute & storage

 - Apache Kafka is a way for you to stream data
 - An Apache cluster is made up of multiple "brokers"
 - Apache Kafka has Producers(your code), such as:
    1. Kinesis
    2. IoT
    3. RDS
    4. etc...
 - The producers send the data to a topic, which is replicated between all brokers
 - Consumers(your code) are:
    1. EMR
    2. S3
    3. SageMaker
    4. Kinesis
    5. RDS
    6. etc...

 - Kinesis Data Streams vs Amazon MSK:
    1. Kinesis Data Streams:
       - 1 MB message size limit
       - Data Streams with Shards
       - Shard Splitting & Merging
       - TLS in-flight encryption
       - KMS at-rest encryption
    2. Amazon MSK:
       - 1 MB default, configure for higher (e.g. 10 MB)
       - Kafka Topics with Partitions
       - Can only add partitions to a topic
       - PLAINTEXT or TLS In-flight Encryption
       - KMS at-rest encryption

 - Amazon MSK Consumers:
    1. Kinesis Data Analytics for Apache Flink
    2. AWS Glue Streaming ETL Jobs Powered by Apache Spark Streaming
    3. Lambda
    4. Any applications running on EC2/ECS/EKS

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Big Data Ingestion Pipeline

 - We want the ingestion pipeline to be fully serverless
 - We want to collect data in real time
 - We want to transform the data
 - We want to query the transformed data using SQL
 - The reports created using the queries should be in S3
 - We want to load that data into a warehouse and create dashboards

 - IoT Devices (can be managed by IoT Core)
 - We can have the below Pipeline diagram

   IoT Devices ---(real-time)---> Amazon Kinesis Data Streams ------> Amazon Kinesis Data Firehose ------> Amazon S3(Ingestion Bucket) ------> Amazon SQS queue(optional) ------ Lambda ---(trigger)---> Athena ---(pull data)---> S3 (ingestion) bucket ------> S3 (reporting) bucket

 - It is possible to also manipulate the data using a Lambda function, which is linked directly to Firehose (in the above diagram)
 - We can visualize the data from the reporting bucket with Amazon QuickSight
 - We can also store the data in Amazon Redshift

 - Big Data Ingestion Pipeline discussion:
 - IoT Core allows you to harvest data from IoT devices
 - Kinesis is great for real-time data collection
 - Firehose helps with data delivery to S3 in near real-time (1 minute)
 - Lambda can help Firehose with data transformations
 - Amazon S3 can trigger notifications to SQS
 - Lambda can subscribe to SQS (we could have connected to S3 to Lambda)
 - Athena is a serverless SQL service and results stored in S3
 - The reporting bucket contains analyzed data and can be used by reporting tools such as QuickSight, Redshift, etc...

