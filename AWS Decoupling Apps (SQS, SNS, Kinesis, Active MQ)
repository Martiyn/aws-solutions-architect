Introduction to messaging:

 - AWS Integration & Messaging:
 - When we start deploying multiple apps, they will inevitably need to communicate with each other.
 - There are two patterns of app communication:
    1. Synchronous communication:
       - application to application
       - Buying service <---------------> Shipping service
    2. Asynchronous / Event based communication:
       - application to queue to application
       - Buying service -----> Queue -----> Shipping service

 - Synchronous between applications can be problematic if there are suddens traffic spikes
 - In case you need to encode 1000 videos but usually it's 10 (this would be a major issue)
 - In such cases, it is much better to decouple the applications using:
    1. SQS: queue model
    2. SNS: pub/sub model
    3. Kinesis: real-time streaming model
 - These services can scale independently from the application

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Amazon SQS (simple queue service)

 - A Queue is the core of SQS
 - An SQS queue needs a so-called "producer" (something to send messages to the queue)
 - You can have multiple producers sending messages to the same queue
 - Messages can be customized to be whatever you want (e.g. "process video" or "process order")
 - On the other side of the SQS queue, you have the so-called "consumers"
 - Consumers poll the messages from the queue, after which they process them and delete them from the queue
 - The SQS queue is a service, which is used to decouple your producers and consumers
 - Producer ---(send messages)---> SQS Queue ---(poll messages)---> Consumer

 - Amazon SQS - Standard Queue:
 - Oldest Amazon offering (over 10 years old)
 - Fully managed service, used to decouple applications
 - Attributes:
    1. Unlimited throughput, unlimited number of messages in queue
    2. Default retention of messages: 4 days, maximum of 14 days
    3. Low latency (<10 ms on publish and receive)
    4. Limitation of 256KB per message sent
 - Can have duplicate messages (at least once delivery, occasionally)
 - Can have out of order messages (best effort ordering)

 - SQS - Producing messages:
 - Produced to SQS using the SDK (SendMessage API)
 - The message is persisted in SQS until a consumer deletes it (signalling that it has been processed)
 - Message retention: default 4 days, up to 14 days
 - Example:
    1. Send an order to be processed
    2. Information for the above message would be:
       - Order id
       - Customer id
       - Any other attributes you want
    3. The consumer will then process the message
 - SQS standard: unlimited throughput

 - SQS - Consuming Messages:
 - Consumers (applications that you need to manually write with code)
 - Consumers can run on:
    1. EC2 instances
    2. On-prem servers
    3. AWS Lambda
 - Poll SQS for messages (can receive up to 10 messages at a time)
 - Process messages (e.g.: insert the message in an RDS database)
 - Once message is processed, delete the messages using the DeleteMessage API
 - SQS - Multiple EC2 instances consumers:
    1. Consumers receive and process messages in parallel
    2. At least once delivery (if message is not processed fast enough, it is received by other consumers)
    3. Best-effort message ordering
    4. Consumers delete messages after processing them
    5. We can scale consumers horizontally to improve throughput processing

 - SQS with Auto Scaling Group (ASG):
 - When consumers run on EC2 instances inside an ASG, you can set a metric for scaling
 - Metric can be set with CloudWatch - Queue Length (ApproximateNumberOfMessages)
 - Then an alarm can be set to CloudWatch, which will trigger the ASG to scale out/in
 
 - SQS to decouple between application tiers
 - You can scale application front and back-end per the below:

    requests ---> Front-end web app(in ASG) --(send message)-> SQS queue --(receive messages)-> Back-end processing app(in ASG) --(insert)-> S3 bucket

 - The above solution is efficient and highly scalable

 - SQS - Security:
 - Encryption:
    1. In-flight encryption using HTTPS API
    2. At-rest encryption using KMS keys
    3. Client-side encryption if the client wants to perform encryption/decryption
 - Access controls: IAM policies to regulate access to SQS API
 - SQS Access policies (similar to S3 bucket policies):
    1. Useful for cross-account access to SQS queues
    2. Useful for allowing other services (SNS, S3, ...) to write to an SQS queue

 - Amazon SQS - Message visibility timeout:
 - After a message is polled by a consumer, it becomes invisible to other consumers
 - By default "message visibility timeout" is 30 seconds
 - That means that the message has 30 seconds to be processed
 - After the visibility timeout runs out and the message is not deleted, the message will be received by another consumer
 - A consumer could call the ChangeMessageVisibility API to get more time (to avoid the message being processed twice)
 - If visibility timeout is high (hours), and consumer crashes, re-processing will take time
 - If visibility timeout is too low (seconds), we may get duplicate processing, as the message will be picked up by other consumers
 - Visibility timeout needs to be something reasonable for your application

 - Amazon SQS - Long Polling
 - When a consumer requests messages from the queue, it can optionally "wait" for messages to arrive if there are none left in the queue
 - This is called Long Polling
 - Long polling decreases the number of API calls made to SQS while increasing the efficiency and latency of your application
 - The wait time can be between 1 to 20 sec (20 sec is preferable)
 - Long polling can be enabled at the queue level or at the API level using WaitTimeSeconds

 - Amazon SQS - FIFO Queue:
 - FIFO - first in first out (ordering of messages in the queue)
 - This means that messages will be ordered in the queue and the first message in the queue will be the first message to be processed
 - Limited throughput: 300 msg/s without batching, 3000 msg/s with batching
 - Exactly-once send capability (by removing duplicates)
 - Messages are processed in order by the consumer

 - SQS + Auto Scaling Group (ASG):
 - If the load is too big, some transactions (requests) may be lost
 - SQS can be used as a buffer to DB writes
 - SQS queue is infinitely scalable
 - When used as a buffer, SQS can ensure no transactions are lost
 - SQS can also buffer between application tiers (front and back-end)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Amazon SNS

 - In case you want to send one message to many receivers
 - Direct integration would be:

   Buying service ---(message)----> Email notification, Fraud Service, Shipping Service, SQS Queue

 - Integrating SNS in known as Pub / Sub (publish / subscribe)
 - Pub / Sub integration would be

   Buying Service ---(message)----> SNS Topic --------> Email notification, Fraud Service, Shipping Service, SQS Queue

 - The "event producer" sends messages to only one SNS topic
 - As many "event receivers" (subscriptions) as we want to listen to the SNS topic notifications
 - Each subscriber to the topic will get all the messages (note: new feature to filter messages)
 - Up to 12,500,000 subscribtions per topic
 - 100,000 topics limit
 - SNS can publish to the following subscribers:
    1. Email notifications
    2. SMS & Mobile notifications
    3. HTTP / HTTPS Endpoints
    4. SQS
    5. AWS Lambda
    6. Kinesis Data Firehose
 - Many AWS services can send data directly to SNS for notifications
 - SNS can receive data from the below AWS Services:
    1. CloudWatch Alarms
    2. Auto Scaling Group (notifications)
    3. S3 buckets (events)
    4. AWS Lambda
    5. AWS Budgets
    6. DynamoDB
    7. RDS Events
    8. AWS DMS (New Replic)
    9. CloudFormation (State Changes)
    10. Any other service that can produce a notification
 - AWS SNS - How to publish:
 - Topic publish (using the SDK):
    1. Create a topic
    2. Create a subscribtion (or many)
    3. Publish to the topic
 - Direct Publish (for mobile apps SDK):
    1. Create a platform application
    2. Create a platform endpoint
    3. Publish to the platform endpoint
    4. Works with Google GCM, Apple APNS, Amazon ADM, ...

 - Amazon SNS - Security:
 - Encryption:
    1. In-flight encryption using HTTPS API
    2. At-rest encryption using KMS keys
    3. Client-side encryption if the client wants to perform encryption / decryption
 - Access controls: IAM policies to regulate access to the SNS API
 - SNS Access Policies (similar to S3 bucket policies):
    1. Useful for cross-account access to SNS topics
    2. Useful for allowing other services (S3, ...) to write to an SNS topic

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

SNS + SQS: Fan Out

 - The idea is that you want a message to be sent to multiple SQS queues
 - If you send them individually to SQS queues, issues can arise
 - To solve this, you can push once in SNS and receive the topic in all SQS queues that are subscribed to the topic
 - What it would look like:

   Buying Service --------> SNS Topic ----(fan-out)-----> SQS queue 1 --------> Fraud Service, SQS queue 2 --------> Shipping service

 - Push once in SNS, receive in all SQS queues that are subscribers
 - Fully decoupled, no data loss
 - SQS allows for: data persistence, delayed processing and retries of work
 - Ability to add more SQS subscribers over time (scalable)
 - Make sure your SQS queue access policy allows for SNS to write
 - Cross-region delivery: works with SQS queues in other regions

 - We can use the above pattern for other services like S3 events to multiple queues:
 - For the same combination of event type(e.g. object create) and prefix(e.g. images/), you can only have one S3 event rule
 - If you want to send the same S3 event to many SQS queues, you can use fan-out
 - What it would look like:

   S3 object created ----(event)----> Amazon S3 ---------> SNS Topic ----(fan-out)-----> SQS queue, SQS queue, Lambda Function

 - You can also send data directly from SNS to Amazon S3 with Kinesis Data Firehose
 - SNS can send to Kinesis and therefore we can have the following architecture:

   Buying Service ------> SNS Topic --------> Kinesis Data Firehose ---------> Amazon S3 (or any supported KDF destination)

 - Amazon SNS - FIFO Topic:
 - FIFO = first in first out (ordering of messages in the topic)
 - Similar features as SQS FIFO:
    1. Ordering by message group ID (all messages in the same group are ordered)
    2. Deduplication using a deduplication ID or content based deduplication
 - Can have SQS standard and FIFO queues as subscribers
 - Limited throughput (same throughput as SQS FIFO)

 - SQS FIFO + SNS FIFO: Fan Out:
 - In case you need fan out + ordering + deduplication:

   Buying service -------> SNS FIFO Topic -----(fan-out)----> SQS FIFO queue 1 --------> Fraud service, SQS FIFO queue 2 --------> Shipping service

 - SNS - Message Filtering:
 - JSON policy used to filter messages sent to SNS topic's subscribtions
 - If a subscribtion doesn't have a filter policy, it receives every message
 - Only messages that match the policy can be sent from the SNS topic to example: an SQS queue:

   Buying Service ---(new transaction)---> SNS Topic ----(Filter policy 1)--> SQS queue 1, ----(Filter policy 2)--> SQS queue 2

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Amazon Kinesis

 - Kinesis makes it easy to collect, process and analyze streaming data in real time
 - Ingest real-time data such as: App logs, Metrics, Website clickstreams, IoT telemetry data, ...
 - There are four types of Kinesis:
    1. Kinesis Data Streams: capture, process and store data streams
    2. Kinesis Data Firehose: load data streams into AWS data stores
    3. Kinesis Data Analytics: analyze data streams with the SQL or Apache Flink
    4. Kinesis Video Streams: capture, process and store video streams

 - Kinesis Data Streams:
 - It is a way to stream big data in your systems
 - It is made up of multiple shards (must provision ahead of time)
 - Data will be split among all shards
 - Producers send data to the client data stream
 - Producers can be:
    1. Applications
    2. SDK, KPL
    3. Client
    4. Kinesis Agent
 - Producers produce records, which are made up of:
    1. Partition key (defines and determines in which shard the record will go to)
    2. Data blob (is the data itself, up to 1 MB)
 - Data can be sent at a rate of 1 MB/s or 1000 msg/s per shard
 - Once the data is processed by the Kinesis streams, it is then passed to the Consumers
 - Consumers can be:
    1. Apps (can depend on KCL or SDK)
    2. AWS Lambda
    3. Kinesis Data Firehose
    4. Kinesis Data Analytics
 - The consumers consume a record, which contains:
    1. Partition key
    2. Sequence no. (represents where the record was in the shard)
    3. Data blob
 - There are different consumption modes:
    1. 2 MB/s (shared) per shard all consumers
    2. 2 MB/s (enhanced) per shard per consumer
 - How it would look like

   Producers ---(record)---> Kinesis Data Stream ---(record)---> Consumers

 - Retention in the data stream is between 1 day to 365 days
 - Ability to reprocess (replay) data
 - Once data is inserted in Kinesis, it can't be deleted (immutable)
 - Data that shares the same partition goes to the same shard (ordering)
 - Producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent
 - Consumers:
    1. Write your own: Kinesis Client Library (KCL), AWS SDK
    2. Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics

 - Kinesis Data Streams - Capacity Modes:
 - Provisioned mode: 
    1. You choose the number of shards provisioned, scale manually or using API
    2. Each shard gets 1 MB/s in (or 1000 records per second)
    3. Each shard gets 2 MB/s out (classic or enhanced fan-out consumer)
    4. You pay per shard provisioned per hour
 - On-demand mode:
    1. No need to provision or manage capacity
    2. Default capacity provisioned (4 MB/s in or 4000 records per second)
    3. Scales automatically based on observed throughput peak during the last 30 days
    4. Pay per stream per hour & data in/out per GB

 - Kinesis Data Streams Security:
 - Control access / authorization using IAM policies
 - Encryption in flight using HTTPS endpoints
 - Encryption at rest using KMS
 - You can implement encryption/decryption of data on client side (harder)
 - VPC endpoints available for Kinesis to access within VPC
 - Monitor API calls using CloudTrail

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Kinesis Data Firehose

 - Kinesis Data Firehose has the following producers:
    1. Applications
    2. SDK, KPL
    3. Client
    4. Kinesis Agent
    5. Kinesis Data Streams
    6. Amazon CloudWatch (logs & events)
    7. AWS IoT
 - You can transform data from Kinesis Firehose by using AWS Lambda functions (optional)
 - Kinesis Firehose can perform batch writes to the following:
    1. Amazon S3
    2. Amazon Redshift (copy through S3)
    3. Amazon OpenSearch
    4. Datadog, splunk, mongoDB
    5. HTTP Endpoint
 - 1-3 are AWS services that Firehose can write to (MUST KNOW)
 - 4 is third-party vendors

 - Kinesis Firehose is a fully managed service, no administration, automatic scaling, serverless
 - Kinesis Data streams vs Kinesis Data Firehose:
 - Kinesis Data streams:
    1. Streaming service for ingest at scale
    2. Write custom code (producer / consumer)
    3. Real-time (~200ms)
    4. Manage scaling (shard splitting / merging)
    5. Data storage for 1 to 365 days
    6. Supports replay capability
 - Kinesis Data Firehose:
    1. Load straming data into S3 / Redshift / OpenSearch / 3rd party / custom HTTP
    2. Fully managed
    3. Near real-time (buffer time min. 60 sec)
    4. Automatic scaling
    5. No data storage
    6. Doesn't support replay capability

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Data Ordering for Kinesis vs SQS FIFO

 - Ordering data into Kinesis:
 - Imagine you have 100 trucks on the road sending their GPS positions regularly into AWS (each with their own ID)
 - You want to consume the data in order for each truck, so that you can track their movement accurately
 - How should you send that data into Kinesis?:
 - Answer: Send using a "partition key" value of the truck_id
 - The same key will always go to the same shard

 - Ordering data into SQS:
 - For SQS FIFO, if you don't use a group ID, messages are consumed in the order they are sent, with only one consumer
 - The order that messages are sent is the order that they will be consumed in
 - You want to scale the number of consumers, but you want messages to be "grouped" when they are related to each other
 - Then you use a Group ID (similar to a partition key in Kinesis)
 - The more Group IDs you have, the more consumers you can have

 - Kinesis vs SQS ordering:
 - Let's assume 100 trucks, 5 kinesis shards, 1 SQS FIFO
 - Kinesis Data Streams:
    1. On average you'll have 20 trucks per shard
    2. Trucks will have their data ordered within each shard
    3. The maximum amount of consumers in parallel we can have is 5
    4. Can receive up to 5 MB/s of data

 - SQS FIFO:
    1. You only have one SQS FIFO queue
    2. You will have 100 Group ID
    3. You can have up to 100 consumers (due to the 100 Group ID)
    4. You have up to 300 messages per second (or 3000 if using batching)

 - In some cases, it would be better to use FIFO, if you have a dynamic number of consumers based on the number of Group IDs
 - In other cases, it is better to use Kinesis, if you have 10,000 trucks and you need to send a lot of data as well as data ordering per shard
