Amazon S3

 - Amazon S3 is one of the main building blocks of AWS
 - Amazon S3 is advertised as "infinitely scaling" storage
 - Many websites use Amazon S3 as their backbone
 - Many AWS services use Amazon S3 as an integration
 - Use cases include:
    1. Backup and storage
    2. Disaster recovery
    3. Archive
    4. Hybrid cloud storage (on-prem and cloud)
    5. Application hosting
    6. Data lakes & big data analytics
    7. Software delivery
    8. Static website
 - S3 stores files in "buckets", which can be seen as top-level directories
 - S3 files are viewed as "objects"
 - Buckets must have a globally unique name (across all regions and all accounts)
 - Buckets are defined at the region level
 - S3 looks like a global service but buckets are created in a region
 - Naming convention for buckets:
    1. No uppercase, No underscore
    2. 3-63 characters long
    3. Must not be an IP
    4. Must start with lowercase letter or number
    5. Must not start with the prefix xn--
    6. Must not end with the suffix -s3alias

 - Amazon S3 objects:
 - Objects (files) have a key
 - The key of the object is the full path:
    1. s3://my-bucket/my_file.txt (where my_file.txt is the key)
    2. If you want to nest the file in a path, the path becomes the key e.g.
       - s3://my-bucket/my_folder1/another_folder/my_file.txt (where my_folder1/another_folder/my_file.txt is the key)
 - The key is composed of a prefix + object name (per the above, my_folder1/another_folder/ is the prefix)
 - Amazon S3 does not have a concept of "directories" (everything is defined as keys per the above)
 - Keys are very long names that contain slashes ("/"), which resemble paths
 - Object values are the content of the body:
    1. Max object size is 5TB (5000GB)
    2. If uploading more than 5GB, "multi-part upload" must be used
 - Objects can have metadata (list of text key / value pairs - system or user metadata)
 - Objects can have tags (Unicode key / value pair - up to 10) - useful for security / lifecycle
 - Objects can have a Version ID (if versioning is enabled)


 - S3 Security:
 - User-based (IAM policies - which API calls should be allowed for a specific IAM user)
 - Resource based:
    1. Bucket policies - bucket wide rules from the S3 console (allows cross account)
       - JSON based policies
          1. Resources: buckets and objects
          2. Effect: Allow/Deny
          3. Actions Set of API to allow or deny (e.g. GetObject API call)
          4. Principal: The account or user to apply the policy to
          5. Use cases:
             - Grant public access to the bucket
             - Force objects to be encrypted at upload
             - Grant access to another account (Cross account)
       - NOTE: IAM permissions are granted when users want to access the S3 buckets
       - NOTE: IAM roles are used when an EC2 instance requires access to the S3 bucket
       - NOTE: For cross-account access, a bucket policy must be created (which targets the user that is to be allowed access) 
    2. Object access control list (ACL) - control on the object level (can be disabled)
    3. Bucket access control list (ACL) - control on the bucket level (can be disabled)
 - Note: an IAM principal can access an S3 object if:
    1. The user IAM permissions ALLOW it OR the recourse policy ALLOWS it
    2. There is no explicit DENY
 - Encryption: encrypt objects in Amazon S3 using encryption keys
 - There is a bucket setting for blocking public access (to prevent data leaks)


 - S3 static website hosting:
 - S3 can host static websites and have them accessible on the Internet
 - The website URL will be (depending on the region):
    1. http://bucket-name.s3-website-aws-region.amazonaws.com
    1. http://bucket-name.s3-website.aws-region.amazonaws.com
 - The bucket can serve static HTML website pages, which can be integrated with AWS Lambda for a serverless approach
 - If public reads are not enabled, when accessing the bucket website URL, it will throw a 403 forbidden error


 - S3 versioning:
 - You can version your files in Amazon S3
 - It must be enabled at the bucket level
 - Same key overwrite will change the "version": 1, 2, 3, ...
 - Best practice is to version the buckets because:
    1. Versioning protects against unintended deletes
    2. It is easy to roll back to a previous version

 - NOTES:
    1. Any file that is not versioned prior to enabling versioning will have version "null"
    2. Suspending versioning does not delete the previous versions
    3. When a file/version gets deleted, it is only marked as deleted (delete marker can be deleted to resore the file)


 - S3 replication:
 - There are two types of replication:
    1. Cross-region replication (CRR)
    2. Same-region replication (SRR)
 - Versioning must be enabled in both source and destination buckets
 - Buckets can be in different AWS accounts
 - Copying is asynchronous
 - Proper IAM permissions must be given to S3
 - Use cases can be the following:
    1. CRR - compliance, lower latency access, replication across accounts
    2. SRR - log aggregation, live replication between prod and test accounts
 - NOTES:
    1. After replication is enabled, only new objects are replicated
    2. Optionally, you can replicate existing objects using S3 batch replication
       - replicate existing objects and objects that failed replication
    3. For DELETE operaions
       - Can replicate delete markers from source to target (optional setting)
       - Deletions with a version ID cannot be replicated (to avoid malicious deletes)
       - Permanent deletions cannot be replicated
    4. There is no "chaining of replication""
       - If bucket 1 has replication into bucket 2, which has replication into bucket 3
       - Objects created in bucket 1 are not replicated to bucket 3


 - S3 storage classes:
 - Amazon S3 Standard - General purpose:
    1. 99.99% availability
    2. Used for frequently accessed data
    3. Low latency and high throughput
    4. Sustain 2 concurrent facility failures
    5. Use cases:
       - Big data analytics, mobile & gaming apps, content distribution

 - Amazon S3 Standard-Infrequent access (IA)
    1. For data that is less frequently accessed, but requires rapid access when needed
    2. Lower cost than S3 standard
    3. 99.9% availability
    4. Use cases:
       - Disaster recovery
       - Backups

 - Amazon S3 One zone-Infrequent access
   1. High durability (99.99999999999%) in a single AZ
   2. Data is lost when AZ is destroyed
   3. 99.5% Availability
   4. Use cases:
      - Storing secondary backup copies of on-premise data or data you can recreate
  
 - Amazon S3 Glacier instant retrieval
    1. Millisecond retrieval, great for data accessed once a quarter
    2. Minimum storage duration of 90 days

 - Amazon S3 Glacier flexible retrieval
    1. Expedited (1 to 5 minutes) - $, Standard (3 to 5 hours) - $, Bulk (5 to 12 hours) - free
    2. Minimum storage duration of 90 days

 - Amazon S3 Glacier deep archive
    1. Standard (12 hours), Bulk (48 hours)
    2. Minimum storage duration of 180 days

 - Amazon S3 intelligent tiering
    1. Move objects between Access tiers based on usage
    2. No retrieval charges in S3 intelligent-tiering
    3. Small monthly monitoring and auto-tiering fee
    4. Tiers are:
       - Frequent access tier (automatic): default tier
       - Infrequent access tier (automatic): objects not accessed for 30 days
       - Archive instant access tier (automatic): objects not accessed for 90 days
       - Archive access tier (optional): configurable from 90 days to 700+ days
       - Deep archive access tier (optional): config. from 180 days to 700+ days

 - You can move between classes manually or by using S3 lifecycle configurations
 - Durability and availability:
    1. Durability:
       - High durability (99.99999999999% 11 9's) of objects across multiple AZ
       - If you store 10,000,000 objects with Amazon S3, you can on average expect to incur loss of a single object once every 10,000 years
       - Durability is the same for all storage classes
    2. Availability:
       - Measures how readily available a service is
       - Varies depending on storage class
       - e.g. S3 standard has 99.99% availability (not available only 53 minutes a year)


 - You can move buckets between storage classes
 - Moving between storage classes is as follows:
    1. Standard can move to:
       - Standard IA
       - Intelligent tiering
       - One-zone IA
       - Glacier Instant retrieval
       - Glacier Flexible retrieval
       - Glacier Deep archive
    2. Standard IA can move to:
       - Intelligent tiering
       - One-Zone IA
       - Glacier Instant retrieval
       - Glacier flexible retrieval
       - Glacier deep archive
    3. Intelligent tiering can move to:
       - One-zone IA
       - Glacier instant retrieval
       - Glacier flexible retrieval
       - Glacier deep archive
    4. One-zone IA can move to:
       - Glacier flexible retrieval
       - Glacier deep archive
    5. Glacier instant retrieval can move to:
       - Glacier flexible retrieval
       - Glacier deep archive
    6. Glacier flexible retrieval can move to:
       - Glacier deep archive

 - For infrequently used objects, you can move them to Standard IA
 - For archive objects that you don't need fast access to, move them to Glacier or Glacier deep archive
 - Moving objects can be automated using Lifecycle rules
 - Lifecycle rules can be the following:
    1. Transition actions:
       - Configure objects to transition to another storage class
       - Move objects to Standard IA class 60 days after creation
       - Move to Glacier for archiving after 6 months
    2. Expiration actions:
       - Configure objects to expire (delete) after some time
       - Access log files can be set to delete after a 365 days
       - Can be used to delete old versions of files (if versioning is enabled)
       - Can be used to delete incomplete Multi-part uploads

 - NOTE: Rules can be created for a certain prefix
 - NOTE: Rules can be created for certain object tags

 - Example scenario for lifecycle rules:
 - Your application on EC2 creates images thumbnails after profile photos are uploaded
   to Amazon S3. These thumbnails can be easily recreated, and only need to be kept for 60 days.
   The source images should be able to be immediately for these 60 days, and afterwards, the user
   can wait up to 6 hours. How would you design this?:

 - Answers:
    1. S3 source images can be on Standard, with a lifecycle configuration to transition them to Glacier after 60 days.
    2. S3 thumbnails can be One-Zone IA, with a lifecycle configuration to expire (delete) them after 60 days.

 - Example Scenario 2:
 - A rule in your company states that you should be able to recover your deleted S3 objects immediately for 30 days,
   although this may happen rarely. After this time, and for up to 365 days, deleted objects should be recoverable
   within 48 hours.

 - Answers:
    1. Enable S3 versioning in order to have object versions, so that "deleted objects" are in fact hidden by a "delete marker" and can be recovered
    2. Transition the "noncurrent versions" of the object to Standard IA
    3. Transition afterwards the "noncurrent versions" to Glacier Deep Archive

 - Storage class Analysis:
 - Storage class analysis helps you decide when to transition objects to the right storage class
 - Recommendations for Standard and Standard IA (does NOT work for One-Zone IA or Glacier)
 - Report is updated daily
 - 24 to 48 hours to start seeing data analysis
 - It is a good first step to put together lifecycle rules (or improve them)
 - Storage class analysis generates .csv reports.


 - S3 Requester pays:
 - In general, bucket owners pay for all Amazon S3 storage and data transfer costs associated with their bucket
 - With requester pays buckets, the requester pays the cost of the request and the data download from the bucket instead of the bucket owner
 - Helpful when you want to share large datasets with other accounts
 - The requester must be authenticated in AWS (cannot be anonymous)


 - S3 Event Notifications:
 - Event notifications can be made for events such as:
    1. S3:ObjectCreated
    2. S3:ObjectRemoved
    3. S3:ObjectRestore
    4. S3:Replication
 - You can create as many "S3 events" as desired
 - S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer
 - Event notifications require IAM permissions in order to work
 - An SNS Resource (Access) Policy must be created for the specified service
 - The S3 service sends data to a certain service (e.g. SNS, SQS, Lambda), hence the need for the resource access policy
 - Since S3 sends the notifications to the services, it must have explicit access given by the policy

 - Event notifications with Amazon EventBridge:
 - Events happen in the bucket -(all events)-> Amazon EventBridge -(rules)-> Over 18 AWS services as destinations
 - EventBridge gives advanced filtering options with JSON rules (metadata, object size, name, etc..)
 - Multiple destinations - e.g. Step Functions, Kinesis Streams / Firehose
 - EventBridge capabilities - Archive, Replay events, Reliable delivery


 - S3 Baseline performance:
 - Amazon S3 automatically scales to high request rates, latency 100-200ms
 - Your application can achieve at least 3,500 PUT/COPY/POST/DELETE requests per second per prefix in a bucket
 - Your application can achieve at least 5,500 GET/HEAD requests per second per prefix in a bucket
 - There are no limits to the number of prefixes in a bucket.
 - Prefix example:
    1. bucket/folder1/sub1/file
    2. bucket/folder1/sub2/file
    3. bucket/folder1/sub3/file
    4. bucket/folder1/sub4/file
 - Prefix is everything between bucket and file
 - If you spread reads across the prefixes evenly, you can achieve 22,000 requests per second for GET and HEAD
 - To optimize uploads, you can do multi-part uploads across multiple prefixes
 - Multi-part uploads are recommended for files > 100MB and mandatory for files > 5GB
 - Multi-part upload parallelizes uploads to speed up transfers (makes the file into small chunks and uploads them in paralel)

 - S3 Transfer acceleration:
 - Increase the transfer speed by transferring a file to an AWS edge location
 - Edge locations forward the data to an S3 bucket in the target region
 - Transfer acceleration is compatible with multi-part upload

 - S3 Byte-range fetches:
 - You can parallelize GETs by requesting specific byte ranges
 - This provides better resilience in case of failures
 - Byte-range fetches can be used to speed up downloads (by breaking down the download into parts of bytes)
 - Byte-range fetch can be used to retrieve only partial data (by breaking down the download and retrieving only one part)


 - S3 Select & Glacier select:
 - You can retrieve less data using SQL by performing server-side filtering
 - Can filter by rows & columns (simple SQL statements)
 - Less network transfer, less CPU cost client-side
 - S3 select is up to 400% faster and up to 80% cheaper


 - S3 batch operations:
 - With S3 batch operations, you can perform bulk operations on existing S3 objects with a single request
 - Example:
    1. Modify objects metadata and properties in bulk
    2. Copy objects between S3 buckets
    3. Encrypt un-encrypted objects
    4. Modify ACLs, tags
    5. Restore objects from S3 Glacier
    6. Invoke Lambda function to perform custom action on each object
 - A job consists of a list of objects, the action to perform and optional parameters
 - S3 Batch operations manages retries, tracks progress, sends completion notifications, generates reports
 - You can use S3 inventory to get object list and use S3 Select to filter your objects


 - AWS S3 object encryption:
 - You can encrypt objects in S3 buckets using one of 4 methods:
    1. Server-Side encryption (SSE)
       - SSE with amazon S3-managed keys (SSE-S3) - Enabled by default
          1. Encrypts S3 objects using keys handled, managed, and owned by AWS (you don't have access to this key)
          2. Object is encrypted server-side
          3. Security type is AES-256
          4. Must set header "x-amz-server-side-encryption": "AES256"
          5. Always enabled by default for new buckets & objects

       - Server-side encryption with KMS keys stored in AWS KMS (SEE-KMS)
          1. Leverage AWS key management service to manage encryption keys
          2. KMS advantages: user control + audit key usage using CloudTrail
          3. Object is encrypted server side
          4. Must set header "x-amz-server-side-encryption": "aws:kms"
          5. If you use SSE-KMS, you may be impacted by the limits of KMS
          6. When you upload: it calls the GenerateDataKey KMS Api
          7. When you download, it calls the Decrypt KMS API
          8. Count towards the KMS quota per second (5500, 10000, 30000 req/s based on region)
          9. You can request a quota to increase using the Service Quotas Console

       - Server-side encryption with customer-provided keys (SSE-C)
          1. Server-side encryption using keys fully managed by the customer outside of AWS
          2. Amazon S3 does NOT store the encryption keys you provide
          3. HTTPS must be used
          4. Encryption key must be provided in HTTP headers, for every HTTP request made

    2. Client-side encryption:
       - Encrypt everything client-side and THEN upload it to Amazon S3
       - Use client libraries such as Amazon S3 client-side encryption library
       - Clients must encrypt data themselves prior to sending to Amazon S3
       - Clients must decrypt the data themselves when retrieving Amazon S3
       - Customer fully manages the keys and encryption cycle

    3. Encryption in transit (SSL/TLS)
       - Encryption in flight is also called SSL/TLS
       - Amazon S3 exposes two endpoints:
          1. HTTP endpoint - non encrypted
          2. HTTPS endpoint - encryption in flight
       - HTTPS is recommended
       - HTTPS is mandatory for SSE-C
       - Most clients would use the HTTPS endpoint by default

 - Amazon S3 - Force encryption in transit:
 - It works by making an S3 policy with a condition ("aws:SecureTransport": "false")
 - The above condition disallows HTTP requests to be made to the bucket (it allows only HTTPS)


 - Amazon S3 Cross-origin resource sharing (CORS):
 - Origin = scheme (protocol) + host (domain) + port
 - Example of the above is: https://www.example.com (implied port is 443 for HTTPS, 80 for HTTP)
 - Web browser based mechanism to allow requests to other origins while visiting the main origin
 - Same origin example: http://example.com/app1 & http://example.com/app2 (where example.com is the same origin)
 - Different origins example: http://www.example.com & http://other.example.com (both urls are different)
 - The requests won't be fulfilled unless the other origin allows for the requests
 - Requests are allowed via CORS headers (e.g.: Access-Control-Allow-Origin)
 - CORS headers function per the following:
    1. Web browser makes a request to a Web server (Origin, e.g. www.origin.com)
    2. Origin web server returns a response requesting resources from another origin
    3. Due to browser security, the Web browser will first make a preflight request to the Cross-origin Web server (www.cross-origin.com)
    4. The request to the cross-origin server is made with the origin www.origin.com to the host www.cross-origin.com
    5. If the cross-origin server is configured to use CORS, then it will give a response
    6. Response contains information that it allows the cross origin with the following headers:
       - Access-Control-Allow-Origin: www.origin.com (allowed origin)
       - Access-Control-Allow-Methods: GET, PUT, DELETE, POST (allowed methods)
    7. If the browser accepts the CORS headers, then it will make a true request (not preflight)

 - CORS applies to Amazon S3 in the following way:
    1. If a client makes a cross-origin request on our S3 bucket, we need to enable the correct CORS headers
    2. You can allow for a specific origin or for * (all origins)
 - CORS is a web browser level security tool


 - Amazon S3 - MFA delete:
 - MFA forces users to generate a code on a device before doing important operations on S3
 - MFA will be required to:
    1. Permanently delete an object version
    2. Suspend versioning on the bucket
 - MFA won't be required to:
    1. Enable versioning
    2. List deleted versions
 - To use MFA Delete, versioning must be enabled on the bucket
 - Only the bucket owner (root account) can enable/disable MFA delete
 - MFA delete can only be enabled/disabled via the AWS CLI


 - S3 access logs:
 - For audit purposes, you may want to log all access to S3 buckets
 - Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket
 - The data can be analyzed using data analysis tools (like AWS Athena)
 - The target logging bucket must be in the same AWS region
 - NEVER set your logging bucket to be the monitored bucket (this will create a logging loop, making the bucket grow exponentially)


 - S3 pre-signed URLs:
 - Generate pre-signed URLs using the S3 console, AWS CLI or SDK
 - URL expiration:
    1. S3 console - 1 min up to 720 mins (12 hours)
    2. AWS CLI - configure expiration with "--expires-in" parameter in seconds (default 3600 sec., max. 604800 sec. ~ 168 hours)
 - Users given a pre-signed URL inherit the permissions of the user that generated the URL for GET / PUT requests
 - Examples are:
    1. Allow only logged-in users to download a premium video from your S3 bucket
    2. Allow an ever-changing list of users to download files by generating URLs dynamically
    3. Temporarily allow a user to upload a file to a precise location in your S3 bucket
 - Pre-signed URLs provide access even to private buckets


 - S3 Glacier vault lock:
 - Adopt a WORM (write once read many) model
 - Create a Vault lock policy
 - Lock the policy for future edits (can no longer be changed or deleted)
 - Helpful for compliance and data retention
 - Lock policy on the bucket level


 - S3 Object lock (versioning must be enabled):
 - Adopt a WORM (write once read many) model
 - Block an object version deletion for a specified amount of time
 - Lock policy on the objects level (policies can be applied to each object individually)
 - There are two retention modes:
    1. Compliance:
       - Object versions can't be overwritten or deleted by any user, including the root user
       - Object retention modes can't be changed, and retention periods can't be shortened
    2. Governance:
       - Most users can't overwrite or delete an object version or alter its lock settings
       - Some users have special permissions to change the retention or delete the object
 - Retention period: protect the object for a fixed period, it can be extended
 - Legal hold: 
    1. Protect the object indefinitely, independent from retention period
    2. Can be freely placed and removed using the s3:PutObjectLegalHold IAM permission


 - S3 Access Points:
 - Access points are policies, which can grant read/write permissions to prefixes
 - Because of this, the access does not need to be granted to each file behind the prefix individually
 - Example:
    1. Policy to grant read/write access to /finance prefix (finance access point) --> /finance/ (prefix)
    2. Policy to grant read/write access to /sales prefix (sales access point) --> /sales/ (prefix)
    3. Policy to grant read access to the entire bucket (analytics access point) --> Access to all prefixes
 - Access points are used to simplify security management for S3 buckets (for scaling purposes)
 - Each access point has:
    1. Its own DNS name (internet origin or VPC origin)
    2. An access point policy (similar to bucket policy) - manage security at scale

 - We can define the access point to be accessible only from within the VPC
 - You must create a VPC endpoint to access the access point (gateway or interface endpoint)
 - This VPC endpoint policy must allow access to the target bucket and access point
 

 - S3 Object Lambda:
 - Use AWS Lambda functions to change the object before it is retrieved by the caller application
 - Only one S3 bucket is needed, on top of which, we can create S3 access point and S3 object lambda access points
 - Use cases:
    1. Redacting personally identifiable information for analytics or non-prod. environments
    2. Converting data across data formats (such as converting XML to JSON)
    3. Resizing and watermarking images on the fly using caller-specific details (such as the user who requested the object)
